---
title: "Connecting SOC with RL -- Importance sampling"
author: "Alonso Cisneros"
institute: Zuse Institute Berlin
format:
    revealjs:
        output-file: index.html
        incremental: true
        html-math-method: mathjax
        theme: zib.scss
        default-image-extension: png
        mermaid-format: svg
        multiplex: true
        title-slide-attributes: 
          data-background-color: "#000099"
        show-notes: separate-page
    beamer:
        papersize: a4
        fig-align: center
        default-image-extension: pdf
        header-includes:
        - \logo{\ifnum\thepage>1\includegraphics[width=1cm]{img/zib/ZIB_Logo_small.png}\fi}
        - \titlegraphic{\includegraphics[width=2.5cm]{img/zib/ZIB_Logo_blue.png}}

bibliography: refs.bib
lang: en-us
self-contained: true
lightbox: false
engine: julia
julia: 
  exeflags: ["--project"]
nocite: |
  @*
---

#

![](img/diagram1.svg)

:::{.notes}
By the end of the talk these will all be connected. What I want to do is build the
connection to yet another blob: ABMs
:::

-----

![](img/diagram3.svg)


# Outline

:::{.nonincremental}
1. _Crash_ course on RL

2. What is importance sampling
  - The connection to optimization
  - Optimal biasing

4. Optimal biasing as an RL problem

5. The things I'd like to connect
:::

# Crash course on Reinforcement Learning

![A miniopoly board](img/board.svg)

:::{.notes}
- I'm training a robot to become the best Miniopoly player
- The rules:
  - Players play in turns
  - They move a number of squares determined by a 4-sided dice roll
  - After completing a lap, it gets a reward of $x$ dollars
  - The trap squares do what it says on the square
  - They can buy property and hotels in the squares.
    - If they land of a square someone owns, they pay
    - If someone lands on their square, they charge rent
  - The game ends when someone runs out of money
:::

-----

- The game has a state at turn $t$ denoted $s_t$
- At a turn $t$ players roll the dice
- The change in money after buying/paying rent/charging rent is recorded as a reward $r_t$

. . .

:::{.callout-important}
We train our robot to maximize the rewards as it takes actions exploring the space of
states
:::

:::{.notes}
- The state of the game an any given time is information like, who owns what squares, how
much money they have, in what positions each player is, and so on.
- Once a player lands in another square, they can choose to buy it if available. If it's
not, we carry out the accounting of how much rent is, and let the player know how much it
won/lost and to what square this is connected.
:::

## Dynamics of the space

::: {.r-stack}
:::{.fragment .fade-out}
![](img/transicion.svg){.r-stretch}
:::

:::{.fragment}
![](img/transicion-markov.svg){.r-stretch}
:::
:::

:::{.notes}
In this example we have full access to the dynamics of the problem. This is not always
the case
:::

-----

![](img/diagram_rl.svg)

:::{.notes}
- We just reviewed very quickly the first bubble
:::

## What if we don't know how square transitions work?

We calculated transition probability _with_ the knowledge of the dice

:::{.notes}
We'll now move on to MCMC. These concepts are not necessarily related, but I will take
advantage of this example so that we can have a reasonable inutition from the start.
:::


## Markov Chain Monte Carlo

- We let the robot roam around and buy squares as it pleases
  - For any square, it can either buy it or not
- We register what it gained or lost by buying or not buying a square by the end of the
  game.

::: {.fragment .center}
![](img/hist-miniopoly.svg)
:::

:::{.notes}
- Graph:
  - This is a violin plot. It shows the estimated probability densities of observing...
  - The x axis shows the different squares
  - The y axis shows the estimated rewards. i.e. money
  - The red distributions correspond to the expected reward when buying a square, while
    the blue the expected loss when not buying them
  - i.e. When we buy squares we expect to profit from them, but clearly not all squares
    are as profitable, look at the different shapes of the distributions. On the other hand,
    it looks like losing any given square leads to the same expected loss
:::

-----

![](img/diagram_IS.svg)

:::{.notes}
Now we have introduced MCMC as a way for our robot to explore it's environment and
estimate which moves are beneficial to his goal. This is what the diagram is representing


Moving on...
:::

# Importance Sampling

- We wanted to compute the expected reward of the robot after the entire game
- MCMC often fails in metastable systems
- Importance sampling aims to remedy this

:::{.fragment .callout-important}
The general idea of importance sampling is to draw random variables from another
probability measure and subsequently weight them back in order to still have an unbiased
estimator of the desired quantity of interest
:::

:::{.notes}
- We **estimated** this quantity by observing and measuring an empirical average. But our
  approximation for extremely unlikely states will always be bad by virtue of how little
  samples we have.
- Metastability makes MCMC extremely hard to apply. The variance of our estimations is
  always going to be enormous under these conditions
- We can aim to make sampling faster by reducing the inherent variance
- **After Callout** In the case of stochastic processes this change of measure corresponds
  to adding a control to the original process
:::


## More formally...

- The metastable stochastic system we sample from follows Langevin dynamics
\begin{equation}
  \mathrm{d}X_s = - \nabla V(X_s) \, \mathrm{d}s + \sigma(X_s) \, \mathrm{d}W_s
\end{equation}
- We want to hit a target set $\mathcal{T}$. We define
\begin{equation}
  \tau = \inf \{ s > 0 \mid X_s \in \mathcal{T} \}
\end{equation}
- We're interested in computing $I: C([0, \infty), \mathbb{R}^d) \to \mathbb{R}$
\begin{equation}
  I(X) \coloneqq \exp(- \mathcal{W}(X))
\end{equation}

:::{.notes}
- Where:
  - $X_s$ is the position of our particle at time $s$
  - $V$ is a "potential" 
  - We assume there exists a unique strong solution that is ergodic
- Note that $\tau$ is a.s. finite
- Where $\mathcal{W}$ serving as a measure of "work" over a trajectory
:::


-----

Our main goal is to compute
$$
  \Psi(X) \coloneqq \mathbb{E}^x [I(X)] \coloneqq \mathbb{E}[I(X) \mid X_0 = x]
$$

But...

. . .

- MCMC has terrible properties because of metastability
- Closed forms of $\Psi(X)$ maybe don't exist

. . .

:::{.callout-tip}
 - We can "push" the particle adding force, as long as we account for it and correct for
 that bias
- That "push" is achieved by adding a control $u$.
:::

-----

The new, controlled dynamics are now described as
\begin{equation}
\label{eq: controlled langevin sde}
\mathrm dX_s^u = (-\nabla V(X_s^u) + \sigma(X_s^u)  \,  u(X_s^u))\mathrm ds + \sigma(X_s^u) \mathrm dW_s, \qquad X_0^u = x 
\end{equation}

. . .

Via Girsanov, we can relate our QoI to the original as such:
\begin{equation}
\label{eq: expectation IS}
    \mathbb{E}^x\left[I(X)\right] = \mathbb{E}^x\left[I(X^u) M^u\right],
\end{equation}

. . .

where the exponential martingale
\begin{equation}
\label{eq: girsanov martingale}
M^u \coloneqq \exp{\left(-  \int_0^{\tau^u} u(X_s^u) \cdot \mathrm dW_s - \frac{1}{2} \int_0^{\tau^u} |u(X_s^u)|^2 \mathrm ds \right)}
\end{equation}
corrects for the bias the pushing introduces.

-----

:::{.callout-important}
The previous relationship always holds. But the variance of the estimator depends
_heavily_ on the choice of $u$.
:::

. . .

Clearly, we aim to achieve the smallest possible variance through on _optimal control_
$u^*$
\begin{equation}
\label{eq: variance minimization}
\operatorname{Var} \left( I(X^{u^*}) M^{u^*} \right)
= \inf_{u \in \mathcal{U}} \left\{ \operatorname{Var} (I(X^u) M^u) \right\}
\end{equation}


:::{.notes}
- Where:
  - $X^{u}_{s}$ is the position of our particle at time $s$ under control $u$
  - The potential $u$ is an It√¥ integrable function satisfying a linear growth condition
- Note that $\tau$ is a.s. finite
- Where $\mathcal{W}$ serving as a measure of "work" over a trajectory
:::

## Connection to optimization


It turns out ^[Feynman--Kac $\to$ Hopf--Cole Transformation $\to$ Hamilton-Jacobi-Bellman]
that the problem of minimizing variance corresponds to a problem in optimal control

. . .

The cost functional $J$ to find the variance minimizing control is
\begin{equation}
\label{eq: cost functional}
J(u; x) \coloneqq \mathbb{E}^x\left[\mathcal{W}(X^u) + \frac{1}{2} \int_0^{\tau^u} |u(X_s^u)|^2 \mathrm ds \right],
\end{equation}

-----

With this formulation,
\begin{equation}
    \Phi(x) = \inf_{u \in \mathcal{U}} J(u; x).
\end{equation}


:::{.callout-important}
The optimal bias achieves zero variance:
\begin{equation}
    \operatorname{Var} \left( I(X^{u^*}) M^{u^*} \right) = 0.
\end{equation}
:::

# Optimal biasing through RL

![](img/diagram_complete.svg)

------

- Let's reconsider the SOC problem (excuse the change in notation)
<!-- - We discretize with Euler--Maruyama -->
- We discretize the dynamics equation
\begin{align*}
s_{t+1} &= s_t + \left( -\nabla V(s_t)  + \sigma u(s_t)\right) \Delta t + \sigma \, \sqrt{\Delta t} \, \eta_{t+1} \\
s_0 &= x
\end{align*}

:::{.notes}
- Sorry for the slightly different notation
- Where
  - Our state is now represented by $s$
  - We have the same potential $V$
  - The diffusion term is $\sigma$ again
  - $\Delta t$ is the length of the time step
  - The term $\sqrt{\Delta t} \eta_{t+1}$ is a Brownian increment, $\eta_t \sim N(0, 1)$
:::

-----

The time-discretized objective function is given by
\begin{equation}
\small
J(u; x) \coloneqq \mathbb{E}^{x} \left[ g(s_{T_u}) + \sum_{t=0}^{T_{u-1}} f(s_t) \Delta t + \frac{1}{2} \sum_{t=0}^{T_{u-1}} |u(s_t)|^2 \Delta t \right]
\end{equation}

- Our stopping time $\tau$ is now denoted $T_u$

## Some formalities

- The state space $\mathcal{S}$ is all possible $s \in \mathbb{R}^d$
- The action space $\mathcal{A}$ is the codomain of all possible controls $\mathbb{R}^d$
- The stopping time $T_u$ for the controlled process is a.s. finite

. . .

- We'll approximate the control with Galerkin projections $u_\theta$
- We still need to derive probability transition and reward functions

-----

- The return we want to optimize depends on a rewards function
$$
r_t = r(s_t, a_t) \coloneqq
\begin{cases}
  - f(s_t) \Delta t - \frac{1}{2} |a_t|^2 \Delta t & \text{if} \; s_t \notin \mathcal{T} \\
  -g(s_t) & \text{if} \quad s_t \in \mathcal{T}.
\end{cases}
$$

:::{.notes}
- The reward function is defined such that the corresponding return along a trajectory
equals the negative term inside the expectation of the time-discretized cost functional
- Notice that the reward signal is in general not sparse since the agent receives feedback
at each time step but the choice of the running cost f and the final cost g can influence
this statement.
:::

-----

![](img/diagram_complete.svg)

# What I want to do

::: {.nonincremental}
- The connection [@BorrellConnecting24] works because of the properties of $J$

- Two posibilities
  1. Take the SOC already published in [@HelfmannInfluencers2023] and pose the _right_
    cost functional (the "easy one")
  2. Go back to the MFE and then
    - Break the assumption of fully connected Agent-Agent network
    - Find the MFE without that assumption
    - Pose a SOC
    - Find the right cost functional
:::

:::{.notes}
- Half of the magic is the fact that the functional to be optimized to solve the optimal
  biasing SOC problem leads to HJB. It's an open question whether I can make our SOC
  compatible
:::

## What I'm trying at the moment

::: {.nonincremental}
- Approach it from the MFE side
  - Looking for generalizations of McKean-Vlasov PDEs with time-dependent networks
  - Thinking what graphon I can get from the network

- Finding the right cost functional on the "easy" case
:::

## Bonus: Assortativity & network topology

::: {.panel-tabset}

### Start
![](img/assort_1.svg){.center}

### End
![](img/assort_195.svg){.center}

### End (echo chamber)
![](img/assort_195_ec.svg){.center}
:::

# References
